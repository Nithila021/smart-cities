================================================================================
RESEARCH 3: UNIFIED SYSTEM DESIGN & IMPLEMENTATION PLAN
Combining Augment Analysis + ChatGPT Architecture Recommendations
================================================================================

================================================================================
IMPLEMENTATION STATUS SNAPSHOT (as of 2026-02-01)
================================================================================

This document is the **design source of truth**. Below is a quick status view
of how much of it is already reflected in the current repo.

- DATA LAYER
  - Implemented: CSV-based NYPD cleaner (`backend/data_cleaner.py`) and
    in-memory cache (`backend/data_init.py`).
  - Implemented: PostgreSQL + PostGIS instance with NYPD crime data migrated
    (manual setup; app still reads from CSV for now).
  - Planned: Move schema into code (`schema.sql`, `migrate_data.py`,
    `db_config.py`) and wire Flask to read from Postgres instead of CSV.
  - Planned: Real amenities via Overpass, 311 complaints, lighting, transit.

- ML LAYER
  - Implemented: K-Means / DBSCAN / KDE clustering & density
    (`backend/models.py`).
  - Planned: Crime-type RiskVector, time-decay weighting, XGBoost,
    confidence intervals, proper feature importance.

- DEMOGRAPHIC PERSONALIZATION
  - Implemented: Demographic impact weights and personalized text
    recommendations (`backend/analysis.py`, `backend/demographic_parser.py`).
  - Planned: Exposure multipliers, explicit `PersonalizedRisk` pipeline,
    and new `/personalized-risk`, `/compare-places`, `/safe-times` endpoints.

- LLM + VISUALIZATION
  - Planned: LLM explanation service and advanced Mapbox / uncertainty
    visualization on the frontend.

See **PART 4: IMPLEMENTATION PHASES** and **PART 7: CODE MAPPING** below for
more detailed, per-task status.

================================================================================
DATABASE SCHEMA & RISK CACHE DESIGN (DECIDED)
================================================================================

STATUS: Ready for implementation

## SUMMARY OF KEY DECISIONS:

| Decision                     | Choice                          | Rationale                    |
|------------------------------|--------------------------------|------------------------------|
| Amenities cache strategy     | Append + refresh on stale      | Avoid stale data, track provenance |
| Risk cache demographic       | Separate rows per demographic  | Clean queries, indexable     |
| Grid system                  | Simple grid for MVP, H3 later  | Faster to implement now      |
| Cache invalidation           | Hybrid (time + event-based)    | Balance freshness vs compute |
| Store crime counts           | Yes, raw + weighted            | Enables confidence + debug   |

--------------------------------------------------------------------------------
TABLE 1: AMENITIES (Smart POI Cache)
--------------------------------------------------------------------------------

PURPOSE: Dynamic cache for geocoded locations. NOT a manually curated list.

WORKFLOW:
  1. User provides: address, place name, or coordinates
  2. System checks amenities table (cache hit?)
  3. Cache MISS → Query external API → Insert → Return
  4. Cache HIT + fresh (<30 days) → Return cached
  5. Cache HIT + stale (>30 days) → Return cached, queue background refresh

KEY FIELDS (for schema.sql):
- Core: `id` (PK), `name`, `location GEOGRAPHY(POINT,4326)`
- Classification: `amenity_type`, `amenity_tags JSONB`
- External link: `external_id`, `external_api_source`
- Operational: `operating_hours JSONB`, `neighborhood`
- Cache: `created_at`, `last_verified_at`, `verification_count`
- Risk: `risk_modifier` (e.g. 1.2 = increases risk, 0.8 = decreases)

INDEXES:
- GIST index on `location`
- B-tree index on `amenity_type`
- B-tree index on (`external_api_source`, `external_id`)

--------------------------------------------------------------------------------
TABLE 2: RISK_CACHE (Demographic-Specific ML-Computed Risk Scores)
--------------------------------------------------------------------------------

PURPOSE: Pre-computed risk scores PER DEMOGRAPHIC GROUP based on ACTUAL
         victimization data for that demographic in that location.

CORE INNOVATION (PATENT-WORTHY):
  - For demographic_group = "women" → count crimes WHERE victim_sex = 'F'
  - For demographic_group = "children" → count crimes WHERE victim_age < 18
  - For demographic_group = "elderly" → count crimes WHERE victim_age >= 65
  - For demographic_group = "general" → count ALL crimes (baseline)

RISK CALCULATION FORMULA:
```
risk_score(location, time, demographic) =
    COUNT(crimes WHERE victim matches demographic
          AND location IN grid_cell
          AND time matches hour_bucket/day_type)
    × time_decay_weight (recent = 1.0, 30-90 days = 0.7, 90+ days = 0.4)
    × ML_adjustments (K-Means zone, DBSCAN hotspot, KDE density)
```

WHY SEPARATE ROWS PER DEMOGRAPHIC (not JSONB):
  - Clean queries: WHERE demographic_group = 'women'
  - Indexable: Can index on demographic_group column
  - Explicit: No parsing JSONB
  - Trade-off: 4x storage (acceptable)

GRID SYSTEM DECISION:
  - MVP: Simple grid using ROUND(lat, 3), ROUND(lon, 3) → ~111m cells
  - Future: H3 hexagons resolution 9 (~174m) when scaling
  - grid_cell_id format: "40.758_-73.985"

CACHE INVALIDATION:
  - Hybrid: time-based (expires_at) + event-based (is_stale flag)
  - Daily job marks cells as stale if new crimes added
  - Background job recomputes stale cells

KEY FIELDS (for schema.sql):
- Identity/location: `id` (PK), `grid_cell_id`, `center_point GEOGRAPHY(POINT,4326)`
- Clustering context: `zone_id`, `dbscan_cluster_id`, `density_level`
- Time context: `hour_bucket`, `day_type`
- Demographic: `demographic_group`
- Counts: `raw_crime_count`, `weighted_crime_count`, `crime_breakdown JSONB`
- Scores: `risk_score`, `confidence`
- Cache: `computed_at`, `expires_at`, `data_version`, `is_stale`

CONSTRAINTS / INDEXES:
- UNIQUE (`grid_cell_id`, `hour_bucket`, `day_type`, `demographic_group`)
- Indexes on `grid_cell_id`, `demographic_group`, (`hour_bucket`, `day_type`)
- Partial index on `is_stale` WHERE `is_stale = TRUE`
- GIST index on `center_point`

TABLE 3: CRIME_INCIDENTS (Core Crime Data) - From Original Design
--------------------------------------------------------------------------------

KEY FIELDS (for schema.sql):
- `id` (PK), `location GEOGRAPHY(POINT,4326)`
- `crime_type`, `crime_category`, `severity_weight`
- `occurred_at`, `day_of_week`, `hour_of_day`
- `victim_demographics JSONB`, `precinct`, `created_at`

INDEXES:
- GIST index on `location`
- B-tree index on `occurred_at`
- B-tree index on `crime_category`

TABLE 4: DEMOGRAPHIC_IMPACT_WEIGHTS (Configurable Multipliers)
--------------------------------------------------------------------------------

PURPOSE: Research-backed multipliers for crimes that disproportionately
         affect certain demographics (e.g., sexual crimes → women = 1.8x)

KEY FIELDS (for schema.sql):
- `id` (PK)
- `crime_category`, `demographic_group`
- `impact_weight`, `source`, `updated_at`

CONSTRAINTS / INDEXES:
- UNIQUE (`crime_category`, `demographic_group`)

--------------------------------------------------------------------------------
OPEN QUESTIONS (To Decide)
--------------------------------------------------------------------------------

[ ] Time-decay weights: Store in DB or compute on-the-fly?
    - Option A: Add decay_weight column to crime_incidents
    - Option B: Compute during risk calculation (current preference)

[ ] Demographic age mappings (NYPD data):
    - Children: <18 only? or include 18-24?
    - Elderly: 65+ only? or include 55-64?

[ ] Multi-city support: Add city_code to tables now or later?

[ ] crime_breakdown in risk_cache: By crime_type (granular) or crime_category (5 types)?

DESIGN PHILOSOPHY (CORE ANCHOR)
--------------------------------------------------------------------------------
"Crime prediction is location-time based. Risk interpretation is demographic-context based."

Your system:
✗ Does NOT predict criminals
✗ Does NOT label people  
✓ DOES estimate how crime patterns affect different user groups differently

================================================================================
PART 1: UNIFIED 6-LAYER ARCHITECTURE
================================================================================

LAYERS (high-level summary):
- **Layer A – Global Crime Intelligence (Data Ingestion)**  
  Historical NYPD crime, 311 complaints, real amenities (Overpass), lighting,
  transit → clean multi-source events stored in PostgreSQL + PostGIS.
- **Layer B – Spatio-Temporal Risk Modeling (ML Engine)**  
  KDE, K-Means, DBSCAN now; later XGBoost and time-decay → continuous
  `Risk(lat, lon, time)` per crime type.
- **Layer C – Crime-Type Risk Decomposition**  
  Outputs `RiskVector(place,time)` = {sexual, harassment, violent assault,
  theft, property}.
- **Layer D – Demographic Impact & Exposure (core idea)**  
  Applies impact matrix + exposure multipliers + uncertainty.
- **Layer E – Personalized Risk Decision Engine**  
  `PersonalizedRisk = Σ CrimeRisk_type × DemographicImpact × ExposureMultiplier`,
  plus a confidence score.
- **Layer F – Explanation & Visualization Layer**  
  LLM explanations, map visualizations, uncertainty, and temporal patterns.

================================================================================
PART 2: DEMOGRAPHIC IMPACT MATRIX (Formalized)
================================================================================

| Crime Type      | Children   | Women      | Elderly    | General    |
|-----------------|------------|------------|------------|------------|
| Sexual Offense  | VERY HIGH  | HIGH       | MEDIUM     | LOW        |
| Harassment      | LOW        | HIGH       | MEDIUM     | MEDIUM     |
| Violent Assault | MEDIUM     | HIGH       | VERY HIGH  | HIGH       |
| Theft/Robbery   | LOW        | MEDIUM     | HIGH       | MEDIUM     |
| Property Crime  | LOW        | LOW        | LOW        | MEDIUM     |

IMPLEMENTATION NOTE:
- This matrix does NOT change actual crime rates
- It changes HOW MUCH a crime type matters to a specific user group
- Weights can be tuned based on research/victimization statistics

EXPOSURE MULTIPLIERS:
| Context                          | Multiplier |
|----------------------------------|------------|
| Woman alone, bar, after 10PM     | 1.5x       |
| Woman in group, bar, after 10PM  | 0.8x       |
| Child with adults, park, daytime | 0.5x       |
| Child alone, anywhere            | 2.0x       |
| Elderly alone, night             | 1.8x       |

================================================================================
PART 3: TECHNOLOGY UPGRADE MAPPING
================================================================================

CURRENT STATE → TARGET STATE

1. DATA LAYER
   CSV files + Pandas      →  PostgreSQL + PostGIS
   Fake amenity data       →  Real OSM data via Overpass API
   NYPD only               →  NYPD + 311 + Lighting + Transit

2. ML LAYER
   K-Means (30 clusters)   →  K-Means + XGBoost ensemble
   DBSCAN                  →  DBSCAN + time-decay weighting
   KDE                     →  KDE with crime-type separation
   No uncertainty          →  Bayesian confidence intervals
   FUTURE                  →  ST-GCN / Ada-GCNLSTM

3. API LAYER
   /safety-score           →  /safety-score + confidence + breakdown
   /hotspots               →  /hotspots + temporal filtering
   NEW                     →  /personalized-risk (demographic-aware)
   NEW                     →  /compare-places (bar vs bar)
   NEW                     →  /safe-times (time window recommendations)

4. FRONTEND LAYER
   React-Leaflet           →  Mapbox GL JS (or keep Leaflet + deck.gl)
   Basic heatmaps          →  Crime-type-specific layers
   No explanations         →  LLM-powered natural language insights
   No uncertainty viz      →  Confidence indicators on scores

================================================================================
PART 4: IMPLEMENTATION PHASES
================================================================================

PHASE 1: DATA FOUNDATION (Week 1-2)
────────────────────────────────────
Priority: HIGH | Effort: MEDIUM

Tasks (status as of 2026-02-01):
[x] Set up PostgreSQL + PostGIS database
    - DB is running locally with PostGIS enabled.
[x] Design schema for crimes, amenities, risk_scores
    - See TABLES 1–4 above.
[x] Migrate existing NYPD CSV data to database
    - ~435k NYPD records loaded into Postgres.
[ ] Implement Overpass API integration for real POI data
[ ] Add 311 complaints data from NYC Open Data
[x] Create data validation and cleaning pipeline
    - Implemented in `backend/data_cleaner.py` and used by `backend/data_init.py`.

PHASE 2: ML MODEL UPGRADES (Week 3-4)
─────────────────────────────────────
Priority: HIGH | Effort: HIGH

Tasks:
[ ] Separate crime analysis by type (create RiskVector)
[ ] Add time-decay weighting to existing models
[ ] Implement XGBoost classifier for crime type prediction
[ ] Add confidence intervals to safety scores
[ ] Create continuous risk surfaces (upgrade from hard clusters)
[ ] Implement feature importance extraction

PHASE 3: DEMOGRAPHIC PERSONALIZATION ENGINE (Week 5-6)
──────────────────────────────────────────────────────
Priority: HIGH | Effort: MEDIUM

Tasks (status as of 2026-02-01):
[x] Implement Demographic Impact Matrix (configurable weights)
    - Implemented in code as DEFAULT_DEMOGRAPHIC_WEIGHTS in `backend/analysis.py`.
      External JSON config file is still TODO.
[ ] Create Exposure Multiplier calculation
[ ] Build PersonalizedRisk formula implementation
[ ] Add user context input (group size, demographics, venue type)
[ ] Create /personalized-risk API endpoint
[ ] Create /compare-places API endpoint
[ ] Create /safe-times API endpoint

NOTE: The current backend already applies demographic-specific weighting and
generates group-aware text recommendations, but the full Phase 3 API surface
is not yet implemented.

PHASE 4: LLM INTEGRATION (Week 7-8)
───────────────────────────────────
Priority: MEDIUM | Effort: MEDIUM

Tasks:
[ ] Set up OpenAI/Claude API integration
[ ] Design prompt templates for safety explanations
[ ] Create explanation generation service
[ ] Implement context-aware recommendation generation
[ ] Add natural language query interface
[ ] Cache common explanations for performance

PHASE 5: VISUALIZATION UPGRADE (Week 9-10)
──────────────────────────────────────────
Priority: MEDIUM | Effort: MEDIUM

Tasks:
[ ] Evaluate Mapbox GL JS vs keeping React-Leaflet + deck.gl
[ ] Implement crime-type-specific map layers
[ ] Add uncertainty visualization (confidence bands/colors)
[ ] Create temporal slider for time-based risk views
[ ] Add 3D visualization for crime density
[ ] Optimize performance for large datasets

================================================================================
PART 5: NEW API ENDPOINTS SPECIFICATION
================================================================================

ENDPOINT: POST /personalized-risk
─────────────────────────────────
- **Purpose**: Personalized risk for a single place, for a specific user
  profile and time.
- **Request fields**:
  - `location`: `{lat, lon}`
  - `user_profile`: demographic, age_group, group_size, group_composition
  - `context`: time, venue_type (bar, park, street, transit),
    planned_duration (minutes)
- **Response fields**:
  - `personalized_risk_score` (0–100) and `confidence`
  - `risk_breakdown` per crime type (base vs adjusted)
  - `safer_alternatives` (safer_time window, time_risk_reduction,
    nearby_safer_venues)
  - short text `explanation`.

ENDPOINT: POST /compare-places
──────────────────────────────
- **Purpose**: Compare multiple candidate places for the same user/time.
- **Request fields**:
  - `places`: list of `{name, lat, lon}`
  - `user_profile`
  - `time`
- **Response fields**:
  - list of `{name, personalized_risk, confidence, dominant_risk_type}`
  - `recommendation` text and `safer_choice` label.

ENDPOINT: GET /safe-times
─────────────────────────
- **Purpose**: Suggest safer time windows for a given place and user profile.
- **Request fields**:
  - `location`
  - `user_profile`
  - `date` (optional; default = typical pattern)
- **Response fields**:
  - `time_windows`: list of `{start, end, risk, label}`
  - `safest_window`
  - explanation text about temporal pattern.

================================================================================
PART 7: CODE MAPPING - What Changed Where (Current Repo)
================================================================================

This section maps the architecture to the **actual Python modules** that exist
in the repo right now, and highlights what is already done vs still TODO.

BACKEND MODULES:

1. data_cleaner.py + data_init.py
   - DONE:
     - Centralized CSV cleaning and validation pipeline.
     - In-memory caching of cleaned DataFrame and derived objects.
   - TODO:
     - Move schema + migration into code (`schema.sql`, `migrate_data.py`).
     - PostgreSQL connection + loaders (`db_config.py`).
     - Overpass API integration for real POIs.
     - 311 complaints data loader.

2. models.py + analysis.py
   - DONE:
     - K-Means / DBSCAN / KDE clustering and density zones.
     - Basic safety scoring and crime breakdown for a location.
   - TODO:
     - Separate analysis by crime_category into a full RiskVector.
     - Time-decay weighting function.
     - Confidence interval calculation for scores.
     - XGBoost (or similar) model for feature importance / prediction.

3. demographic_parser.py (future: personalization_engine.py)
   - DONE:
     - Parsing natural-language group descriptions into a profile.
     - Generating personalized text recommendations based on profile
       and crime types.
   - TODO:
     - DemographicImpactMatrix and ExposureMultiplier as reusable
       classes/utilities.
     - PersonalizedRiskCalculator and PlaceComparator helpers used by
       future endpoints.

4. (future) llm_service.py
   - TODO:
     - OpenAI/Claude integration.
     - Prompt templates for explanations.
     - Explanation caching.

5. app1.py (Flask routes)
   - CURRENT:
     - Existing safety-score style endpoint(s) backed by CSV pipeline.
   - TODO:
     - /personalized-risk endpoint.
     - /compare-places endpoint.
     - /safe-times endpoint.
     - Extend responses to include confidence and richer breakdowns.

FRONTEND FILES TO MODIFY:

1. Map component
   - ADD: Crime-type layer toggles
   - ADD: Confidence visualization
   - ADD: Time slider

2. NEW: PersonalizedRiskForm component
   - ADD: User profile input
   - ADD: Context selection

3. NEW: PlaceComparison component
   - ADD: Side-by-side comparison UI

================================================================================
PART 8: QUICK WINS (Implement This Week)
================================================================================

1. REPLACE FAKE AMENITY DATA (1 day)
   - Use Overpass API to get real bars, parks, transit stops
   - Immediate improvement to data quality

2. ADD CRIME CATEGORY SEPARATION (1 day)
   - Split analysis into: sexual, harassment, assault, theft, property
   - Foundation for demographic impact matrix

3. ADD BASIC CONFIDENCE SCORE (1 day)
   - confidence = f(sample_size, recency)
   - Show users how reliable scores are

4. CREATE DEMOGRAPHIC IMPACT MATRIX CONFIG (1 day)
   - JSON config file with weights
   - Easy to tune without code changes

5. ADD TIME-DECAY TO EXISTING MODELS (1 day)
   - Recent crimes weighted higher
   - More relevant predictions

================================================================================
PART 9: RESEARCH CITATIONS & JUSTIFICATION
================================================================================

Architecture justified by:
- ST-GCN: "Spatio-Temporal Graph Convolutional Networks" (AAAI 2018)
- Ada-GCNLSTM: Adaptive cross-city crime prediction (2024)
- Demographic impact: FBI Victimization Statistics methodology
- Uncertainty: Bayesian Deep Learning for crime prediction (2023)

Key papers to cite:
1. "Crime Prediction Using Machine Learning" - IEEE Smart Cities 2024
2. "Spatiotemporal Crime Hotspot Detection" - ACM SIGSPATIAL 2023
3. "Ethical Considerations in Predictive Policing" - Nature 2022

================================================================================
END OF RESEARCH 3
================================================================================


