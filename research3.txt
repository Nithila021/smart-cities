================================================================================
RESEARCH 3: UNIFIED SYSTEM DESIGN & IMPLEMENTATION PLAN
Combining Augment Analysis + ChatGPT Architecture Recommendations
================================================================================

================================================================================
IMPLEMENTATION STATUS SNAPSHOT (as of 2026-02-02)
================================================================================

This document is the **design source of truth**. Below is a quick status view
of how much of it is already reflected in the current repo.

- DATA LAYER
  ✅ Implemented: CSV-based NYPD cleaner (`backend/data_cleaner.py`) with:
     - Auto-detection of geographic bounds (multi-city support)
     - Crime type standardization and severity weights
     - Crime category mapping for demographic weighting
     - Demographic data cleaning (age, race, sex)
     - Timestamp parsing with derived time features
  ✅ Implemented: In-memory cache (`backend/data_init.py`) with K-Means zones
  ✅ Implemented: PostgreSQL + PostGIS instance with NYPD crime data migrated
     (manual setup; dual-mode code exists but `USE_DATABASE = False`)
  ❌ Planned: Move schema into code (`schema.sql`, `migrate_data.py`,
     `db_config.py`) and wire Flask to read from Postgres instead of CSV
  ❌ Planned: Real amenities via Overpass API, 311 complaints, lighting, transit

- ML LAYER
  ✅ Implemented: K-Means (30 clusters) / DBSCAN (quadrant-based) / KDE density
     (`backend/models.py`)
  ✅ Implemented: Victim demographic zones (25 zones with concentration scores)
  ✅ Implemented: Crime density classification (Low/Medium/High zones)
  ✅ Implemented: Model evaluation module (`backend/model_evaluation.py`) with
     silhouette, Davies-Bouldin, and Calinski-Harabasz metrics
  ❌ Planned: Crime-type RiskVector separation, time-decay weighting, XGBoost,
     confidence intervals, proper feature importance extraction

- DEMOGRAPHIC PERSONALIZATION
  ✅ Implemented: Demographic impact weights (`analysis.py`) with:
     - DEFAULT_DEMOGRAPHIC_WEIGHTS for general/women/children/elderly
     - apply_demographic_weighting() function
     - Dual-mode support (database or in-memory)
  ✅ Implemented: Demographic parser (`backend/demographic_parser.py`) with:
     - Natural language parsing ("3 white women + 2 black men")
     - Vulnerable group detection (children, elderly, women, LGBTQ+, etc.)
     - Risk factor identification (racial bias, gender-based violence, etc.)
     - Custom safety recommendations per group composition
  ❌ Planned: Exposure multipliers, explicit `PersonalizedRisk` pipeline class,
     and new `/personalized-risk`, `/compare-places`, `/safe-times` endpoints

- API LAYER
  ✅ Implemented: Flask app (`backend/app1.py`) with endpoints:
     - `/api/analyze_v1` - Basic safety analysis
     - `/api/analyze_v2` - Enhanced analysis with density classification
     - `/api/chat` - Natural language interface with demographic parsing
     - `/api/heatmap` - Crime heatmap data
     - `/api/density_map` - Density grid data
     - `/api/demographic_zones` - Demographic zone data
     - `/api/dbscan_clusters` - DBSCAN cluster data
     - `/api/map_data` - Combined map visualization data
  ❌ Planned: `/personalized-risk`, `/compare-places`, `/safe-times` endpoints

- UTILITIES
  ✅ Implemented: Geocoding with multi-city support (`backend/geo_utils.py`)
  ✅ Implemented: Safety report generation (`backend/utils.py`)
  ✅ Implemented: Folium map visualization helpers

- LLM + VISUALIZATION
  ❌ Planned: LLM explanation service (OpenAI/Claude integration)
  ❌ Planned: Advanced Mapbox / uncertainty visualization on frontend

See **PART 4: IMPLEMENTATION PHASES** and **PART 7: CODE MAPPING** below for
more detailed, per-task status.

================================================================================
DATABASE SCHEMA & RISK CACHE DESIGN (DECIDED)
================================================================================

STATUS: Ready for implementation

## SUMMARY OF KEY DECISIONS:

| Decision                     | Choice                          | Rationale                    |
|------------------------------|--------------------------------|------------------------------|
| Amenities cache strategy     | Append + refresh on stale      | Avoid stale data, track provenance |
| Risk cache demographic       | Separate rows per demographic  | Clean queries, indexable     |
| Grid system                  | Simple grid for MVP, H3 later  | Faster to implement now      |
| Cache invalidation           | Hybrid (time + event-based)    | Balance freshness vs compute |
| Store crime counts           | Yes, raw + weighted            | Enables confidence + debug   |

--------------------------------------------------------------------------------
TABLE 1: AMENITIES (Smart POI Cache)
--------------------------------------------------------------------------------

PURPOSE: Dynamic cache for geocoded locations. NOT a manually curated list.

WORKFLOW:
  1. User provides: address, place name, or coordinates
  2. System checks amenities table (cache hit?)
  3. Cache MISS → Query external API → Insert → Return
  4. Cache HIT + fresh (<30 days) → Return cached
  5. Cache HIT + stale (>30 days) → Return cached, queue background refresh

KEY FIELDS (for schema.sql):
- Core: `id` (PK), `name`, `location GEOGRAPHY(POINT,4326)`
- Classification: `amenity_type`, `amenity_tags JSONB`
- External link: `external_id`, `external_api_source`
- Operational: `operating_hours JSONB`, `neighborhood`
- Cache: `created_at`, `last_verified_at`, `verification_count`
- Risk: `risk_modifier` (e.g. 1.2 = increases risk, 0.8 = decreases)

INDEXES:
- GIST index on `location`
- B-tree index on `amenity_type`
- B-tree index on (`external_api_source`, `external_id`)

--------------------------------------------------------------------------------
TABLE 2: RISK_CACHE (Demographic-Specific ML-Computed Risk Scores)
--------------------------------------------------------------------------------

PURPOSE: Pre-computed risk scores PER DEMOGRAPHIC GROUP based on ACTUAL
         victimization data for that demographic in that location.

CORE INNOVATION (PATENT-WORTHY):
  - For demographic_group = "women" → count crimes WHERE victim_sex = 'F'
  - For demographic_group = "children" → count crimes WHERE victim_age < 18
  - For demographic_group = "elderly" → count crimes WHERE victim_age >= 65
  - For demographic_group = "general" → count ALL crimes (baseline)

RISK CALCULATION FORMULA:
```
risk_score(location, time, demographic) =
    COUNT(crimes WHERE victim matches demographic
          AND location IN grid_cell
          AND time matches hour_bucket/day_type)
    × time_decay_weight (recent = 1.0, 30-90 days = 0.7, 90+ days = 0.4)
    × ML_adjustments (K-Means zone, DBSCAN hotspot, KDE density)
```

WHY SEPARATE ROWS PER DEMOGRAPHIC (not JSONB):
  - Clean queries: WHERE demographic_group = 'women'
  - Indexable: Can index on demographic_group column
  - Explicit: No parsing JSONB
  - Trade-off: 4x storage (acceptable)

GRID SYSTEM DECISION:
  - MVP: Simple grid using ROUND(lat, 3), ROUND(lon, 3) → ~111m cells
  - Future: H3 hexagons resolution 9 (~174m) when scaling
  - grid_cell_id format: "40.758_-73.985"

CACHE INVALIDATION:
  - Hybrid: time-based (expires_at) + event-based (is_stale flag)
  - Daily job marks cells as stale if new crimes added
  - Background job recomputes stale cells

KEY FIELDS (for schema.sql):
- Identity/location: `id` (PK), `grid_cell_id`, `center_point GEOGRAPHY(POINT,4326)`
- Clustering context: `zone_id`, `dbscan_cluster_id`, `density_level`
- Time context: `hour_bucket`, `day_type`
- Demographic: `demographic_group`
- Counts: `raw_crime_count`, `weighted_crime_count`, `crime_breakdown JSONB`
- Scores: `risk_score`, `confidence`
- Cache: `computed_at`, `expires_at`, `data_version`, `is_stale`

CONSTRAINTS / INDEXES:
- UNIQUE (`grid_cell_id`, `hour_bucket`, `day_type`, `demographic_group`)
- Indexes on `grid_cell_id`, `demographic_group`, (`hour_bucket`, `day_type`)
- Partial index on `is_stale` WHERE `is_stale = TRUE`
- GIST index on `center_point`

TABLE 3: CRIME_INCIDENTS (Core Crime Data) - From Original Design
--------------------------------------------------------------------------------

KEY FIELDS (for schema.sql):
- `id` (PK), `location GEOGRAPHY(POINT,4326)`
- `crime_type`, `crime_category`, `severity_weight`
- `occurred_at`, `day_of_week`, `hour_of_day`
- `victim_demographics JSONB`, `precinct`, `created_at`

INDEXES:
- GIST index on `location`
- B-tree index on `occurred_at`
- B-tree index on `crime_category`

TABLE 4: DEMOGRAPHIC_IMPACT_WEIGHTS (Configurable Multipliers)
--------------------------------------------------------------------------------

PURPOSE: Research-backed multipliers for crimes that disproportionately
         affect certain demographics (e.g., sexual crimes → women = 1.8x)

KEY FIELDS (for schema.sql):
- `id` (PK)
- `crime_category`, `demographic_group`
- `impact_weight`, `source`, `updated_at`

CONSTRAINTS / INDEXES:
- UNIQUE (`crime_category`, `demographic_group`)

--------------------------------------------------------------------------------
OPEN QUESTIONS (To Decide)
--------------------------------------------------------------------------------

[ ] Time-decay weights: Store in DB or compute on-the-fly?
    - Option A: Add decay_weight column to crime_incidents
    - Option B: Compute during risk calculation (current preference)

[ ] Demographic age mappings (NYPD data):
    - Children: <18 only? or include 18-24?
    - Elderly: 65+ only? or include 55-64?

[ ] Multi-city support: Add city_code to tables now or later?

[ ] crime_breakdown in risk_cache: By crime_type (granular) or crime_category (5 types)?

DESIGN PHILOSOPHY (CORE ANCHOR)
--------------------------------------------------------------------------------
"Crime prediction is location-time based. Risk interpretation is demographic-context based."

Your system:
✗ Does NOT predict criminals
✗ Does NOT label people  
✓ DOES estimate how crime patterns affect different user groups differently

================================================================================
PART 1: UNIFIED 6-LAYER ARCHITECTURE
================================================================================

LAYERS (high-level summary):
- **Layer A – Global Crime Intelligence (Data Ingestion)**  
  Historical NYPD crime, 311 complaints, real amenities (Overpass), lighting,
  transit → clean multi-source events stored in PostgreSQL + PostGIS.
- **Layer B – Spatio-Temporal Risk Modeling (ML Engine)**  
  KDE, K-Means, DBSCAN now; later XGBoost and time-decay → continuous
  `Risk(lat, lon, time)` per crime type.
- **Layer C – Crime-Type Risk Decomposition**  
  Outputs `RiskVector(place,time)` = {sexual, harassment, violent assault,
  theft, property}.
- **Layer D – Demographic Impact & Exposure (core idea)**  
  Applies impact matrix + exposure multipliers + uncertainty.
- **Layer E – Personalized Risk Decision Engine**  
  `PersonalizedRisk = Σ CrimeRisk_type × DemographicImpact × ExposureMultiplier`,
  plus a confidence score.
- **Layer F – Explanation & Visualization Layer**  
  LLM explanations, map visualizations, uncertainty, and temporal patterns.

================================================================================
PART 2: DEMOGRAPHIC IMPACT MATRIX (Formalized)
================================================================================

| Crime Type      | Children   | Women      | Elderly    | General    |
|-----------------|------------|------------|------------|------------|
| Sexual Offense  | VERY HIGH  | HIGH       | MEDIUM     | LOW        |
| Harassment      | LOW        | HIGH       | MEDIUM     | MEDIUM     |
| Violent Assault | MEDIUM     | HIGH       | VERY HIGH  | HIGH       |
| Theft/Robbery   | LOW        | MEDIUM     | HIGH       | MEDIUM     |
| Property Crime  | LOW        | LOW        | LOW        | MEDIUM     |

IMPLEMENTATION NOTE:
- This matrix does NOT change actual crime rates
- It changes HOW MUCH a crime type matters to a specific user group
- Weights can be tuned based on research/victimization statistics

EXPOSURE MULTIPLIERS:
| Context                          | Multiplier |
|----------------------------------|------------|
| Woman alone, bar, after 10PM     | 1.5x       |
| Woman in group, bar, after 10PM  | 0.8x       |
| Child with adults, park, daytime | 0.5x       |
| Child alone, anywhere            | 2.0x       |
| Elderly alone, night             | 1.8x       |

================================================================================
PART 3: TECHNOLOGY UPGRADE MAPPING
================================================================================

CURRENT STATE → TARGET STATE

1. DATA LAYER
   CSV files + Pandas      →  PostgreSQL + PostGIS
   Fake amenity data       →  Real OSM data via Overpass API
   NYPD only               →  NYPD + 311 + Lighting + Transit

2. ML LAYER
   K-Means (30 clusters)   →  K-Means + XGBoost ensemble
   DBSCAN                  →  DBSCAN + time-decay weighting
   KDE                     →  KDE with crime-type separation
   No uncertainty          →  Bayesian confidence intervals
   FUTURE                  →  ST-GCN / Ada-GCNLSTM

3. API LAYER
   /safety-score           →  /safety-score + confidence + breakdown
   /hotspots               →  /hotspots + temporal filtering
   NEW                     →  /personalized-risk (demographic-aware)
   NEW                     →  /compare-places (bar vs bar)
   NEW                     →  /safe-times (time window recommendations)

4. FRONTEND LAYER
   React-Leaflet           →  Mapbox GL JS (or keep Leaflet + deck.gl)
   Basic heatmaps          →  Crime-type-specific layers
   No explanations         →  LLM-powered natural language insights
   No uncertainty viz      →  Confidence indicators on scores

================================================================================
PART 4: IMPLEMENTATION PHASES
================================================================================

PHASE 1: DATA FOUNDATION (Week 1-2)
────────────────────────────────────
Priority: HIGH | Effort: MEDIUM

Tasks (status as of 2026-02-02):
[x] Set up PostgreSQL + PostGIS database
    - DB is running locally with PostGIS enabled.
[x] Design schema for crimes, amenities, risk_scores
    - See TABLES 1–4 above.
[x] Migrate existing NYPD CSV data to database
    - ~435k NYPD records loaded into Postgres.
[x] Create data validation and cleaning pipeline
    - Implemented in `backend/data_cleaner.py` (656 lines, production-ready)
    - Auto-detection of geographic bounds (multi-city support)
    - Crime type standardization, severity weights, demographic cleaning
    - Timestamp parsing with derived time features
[ ] Implement Overpass API integration for real POI data
    - Currently using dummy amenity data (100 POIs across 5 NYC boroughs)
[ ] Add 311 complaints data from NYC Open Data
[ ] Create `db_config.py` with PostgreSQL connection pooling
    - Dual-mode code exists in `analysis.py` but `USE_DATABASE = False`
[ ] Create `schema.sql` with final table definitions
[ ] Create `migrate_data.py` for automated database migration

PHASE 2: ML MODEL UPGRADES (Week 3-4)
─────────────────────────────────────
Priority: HIGH | Effort: HIGH

Tasks (status as of 2026-02-02):
[x] Implement K-Means clustering (30 zones)
    - Implemented in `backend/models.py`
[x] Implement DBSCAN clustering with quadrant-based batching
    - Memory-efficient processing of large datasets
[x] Implement KDE crime density classification (Low/Medium/High)
    - Grid-based density estimation with percentile thresholds
[x] Implement victim demographic zones (25 zones)
    - Concentration scores for age, race, sex
[x] Create model evaluation module
    - Silhouette, Davies-Bouldin, Calinski-Harabasz metrics
    - Implemented in `backend/model_evaluation.py` (291 lines)
[ ] Separate crime analysis by crime_category into RiskVector class
    - Currently crime_types dict is returned, but not structured as RiskVector
[ ] Add time-decay weighting to existing models
    - Formula: 1.0x (<30 days), 0.7x (30-90 days), 0.4x (90+ days)
[ ] Implement XGBoost classifier for crime type prediction
[ ] Add confidence intervals to safety scores
    - Currently no uncertainty quantification
[ ] Create continuous risk surfaces (upgrade from hard clusters)
    - Currently using discrete K-Means zones
[ ] Implement feature importance extraction
    - Placeholder values exist in demographic_feature_importance

PHASE 3: DEMOGRAPHIC PERSONALIZATION ENGINE (Week 5-6)
──────────────────────────────────────────────────────
Priority: HIGH | Effort: MEDIUM

Tasks (status as of 2026-02-02):
[x] Implement Demographic Impact Matrix (configurable weights)
    - Implemented as DEFAULT_DEMOGRAPHIC_WEIGHTS in `backend/analysis.py`
    - Supports: general, women, children, elderly
    - Categories: sexual, harassment, assault, theft, robbery
[x] Implement apply_demographic_weighting() function
    - Adjusts safety scores based on crime composition and demographic group
[x] Implement demographic parser for natural language input
    - `backend/demographic_parser.py` (219 lines)
    - Parses: "3 white women + 2 black men", "family with 2 children"
    - Detects vulnerable groups: children, elderly, women, LGBTQ+, pregnant, disabled
    - Identifies risk factors: racial_bias, gender_based_violence, hate_crimes
[x] Generate custom safety recommendations per group
    - generate_custom_safety_recommendations() with group-specific advice
[x] Integrate demographic analysis into /api/chat endpoint
    - Accepts optional `demographics` parameter
    - Returns demographic_profile and custom recommendations
[ ] Create Exposure Multiplier calculation
    - Planned: group size, venue type, time context multipliers
[ ] Build PersonalizedRisk formula implementation as a class
    - Currently implemented as functions, not a reusable class
[ ] Add user context input (group size, demographics, venue type) to all endpoints
    - Currently only /api/chat supports demographics
[ ] Create /personalized-risk API endpoint
    - Planned: POST with location, user_profile, context
[ ] Create /compare-places API endpoint
    - Planned: POST with multiple places, user_profile, time
[ ] Create /safe-times API endpoint
    - Planned: GET with location, user_profile, date
[ ] Move DEFAULT_DEMOGRAPHIC_WEIGHTS to external JSON config file
    - Currently hardcoded in analysis.py

NOTE: The current backend already applies demographic-specific weighting and
generates group-aware text recommendations via the /api/chat endpoint, but the
full Phase 3 API surface (dedicated endpoints) is not yet implemented.

PHASE 4: LLM INTEGRATION (Week 7-8)
───────────────────────────────────
Priority: MEDIUM | Effort: MEDIUM

Tasks:
[ ] Set up OpenAI/Claude API integration
[ ] Design prompt templates for safety explanations
[ ] Create explanation generation service
[ ] Implement context-aware recommendation generation
[ ] Add natural language query interface
[ ] Cache common explanations for performance

PHASE 5: VISUALIZATION UPGRADE (Week 9-10)
──────────────────────────────────────────
Priority: MEDIUM | Effort: MEDIUM

Tasks:
[ ] Evaluate Mapbox GL JS vs keeping React-Leaflet + deck.gl
[ ] Implement crime-type-specific map layers
[ ] Add uncertainty visualization (confidence bands/colors)
[ ] Create temporal slider for time-based risk views
[ ] Add 3D visualization for crime density
[ ] Optimize performance for large datasets

================================================================================
PART 5: NEW API ENDPOINTS SPECIFICATION
================================================================================

ENDPOINT: POST /personalized-risk
─────────────────────────────────
- **Purpose**: Personalized risk for a single place, for a specific user
  profile and time.
- **Request fields**:
  - `location`: `{lat, lon}`
  - `user_profile`: demographic, age_group, group_size, group_composition
  - `context`: time, venue_type (bar, park, street, transit),
    planned_duration (minutes)
- **Response fields**:
  - `personalized_risk_score` (0–100) and `confidence`
  - `risk_breakdown` per crime type (base vs adjusted)
  - `safer_alternatives` (safer_time window, time_risk_reduction,
    nearby_safer_venues)
  - short text `explanation`.

ENDPOINT: POST /compare-places
──────────────────────────────
- **Purpose**: Compare multiple candidate places for the same user/time.
- **Request fields**:
  - `places`: list of `{name, lat, lon}`
  - `user_profile`
  - `time`
- **Response fields**:
  - list of `{name, personalized_risk, confidence, dominant_risk_type}`
  - `recommendation` text and `safer_choice` label.

ENDPOINT: GET /safe-times
─────────────────────────
- **Purpose**: Suggest safer time windows for a given place and user profile.
- **Request fields**:
  - `location`
  - `user_profile`
  - `date` (optional; default = typical pattern)
- **Response fields**:
  - `time_windows`: list of `{start, end, risk, label}`
  - `safest_window`
  - explanation text about temporal pattern.

================================================================================
PART 7: CODE MAPPING - What Changed Where (Current Repo)
================================================================================

This section maps the architecture to the **actual Python modules** that exist
in the repo right now, and highlights what is already done vs still TODO.

BACKEND MODULES:

1. **data_cleaner.py** (656 lines) - PRODUCTION READY ✅
   - DONE:
     - CleaningReport class with detailed step tracking
     - Auto-detection of geographic bounds (works for ANY city)
     - CITY_BOUNDS_CONFIG for NYC, London, Chicago (extensible)
     - Crime type standardization with regex mappings
     - Crime category mapping (sexual, harassment, assault, theft, robbery, property)
     - SEVERITY_WEIGHTS (0.3-1.0 scale) for risk scoring
     - Demographic cleaning (vic_age_group, vic_race, vic_sex)
     - Timestamp parsing with hour_of_day and day_of_week derivation
     - normalize_columns() for handling different CSV formats
     - clean_for_database() method for schema-ready output
     - CLI interface for standalone usage
   - TODO:
     - None (module is complete for current needs)

2. **data_init.py** (186 lines) - PRODUCTION READY ✅
   - DONE:
     - Global cached_data dict with type hints (Dict[str, Any])
     - prepare_data() - CSV loading with data_cleaner integration
     - train_models() - K-Means, DBSCAN, demographic zones, density zones
     - initialize_data() - Top-level initializer
     - load_amenity_data() - Dummy amenity generator (100 POIs across 5 boroughs)
     - Cleaning report storage for debugging
   - TODO:
     - Replace dummy amenities with Overpass API integration
     - Add 311 complaints data loader

3. **models.py** (409 lines) - PRODUCTION READY ✅
   - DONE:
     - initialize_dbscan_clusters() - Quadrant-based batching for memory efficiency
     - predict_dbscan_cluster() - Nearest-neighbor prediction
     - initialize_victim_demographic_zones() - 25 zones with concentration scores
     - predict_demographic_zone() - Zone prediction with optional demographics
     - initialize_crime_density_zones() - KDE with Low/Medium/High classification
     - get_crime_density_classification() - Per-point density + grid data
     - Multi-city support via CITY_BOUNDS_CONFIG
     - Configurable grid resolution (DENSITY_GRID_SIZE = 50)
   - TODO:
     - Separate analysis by crime_category into a full RiskVector class
     - Time-decay weighting function (1.0x <30d, 0.7x 30-90d, 0.4x 90+d)
     - Confidence interval calculation for scores
     - XGBoost model for feature importance / prediction

4. **analysis.py** (272 lines) - PRODUCTION READY ✅
   - DONE:
     - Dual-mode operation (DB_AVAILABLE flag, USE_DATABASE toggle)
     - DEFAULT_DEMOGRAPHIC_WEIGHTS for general/women/children/elderly
     - get_demographic_weights() - Database or fallback to defaults
     - apply_demographic_weighting() - Core personalization logic
     - analyze_safety() - Main analysis function with demographic_group parameter
     - analyze_amenities() - Nearby amenity analysis
     - Time-of-day analysis (morning/afternoon/evening/night)
     - DBSCAN cluster info integration
     - Demographic zone info integration
     - Crime density classification integration
   - TODO:
     - Exposure multipliers (group size, venue type, time context)
     - PersonalizedRiskCalculator class
     - PlaceComparator helper for /compare-places endpoint

5. **demographic_parser.py** (219 lines) - PRODUCTION READY ✅
   - DONE:
     - parse_demographic_group() - Natural language parsing
     - Supports patterns: "3 white women + 2 black men", "family with 2 children"
     - Vulnerable group detection (children, elderly, women, LGBTQ+, pregnant, disabled)
     - Risk factor identification (racial_bias, gender_based_violence, hate_crimes, etc.)
     - generate_custom_safety_recommendations() - Group-specific safety advice
     - Race-based safety considerations
     - Solo vs large group recommendations
   - TODO:
     - Refactor into PersonalizationEngine class
     - Add ExposureMultiplier calculation

6. **app1.py** (301 lines) - PRODUCTION READY ✅
   - DONE:
     - Flask app with CORS configuration
     - `/api/analyze_v1` - Basic safety analysis with optional advanced features
     - `/api/analyze_v2` - Enhanced analysis with density classification
     - `/api/chat` - Natural language interface with demographic parsing
     - `/api/heatmap` - Crime heatmap data (5000 sample points)
     - `/api/density_map` - Density grid data
     - `/api/demographic_zones` - Demographic zone data
     - `/api/dbscan_clusters` - DBSCAN cluster data
     - `/api/map_data` - Combined map visualization data
     - `ensure_data_initialized()` helper
   - TODO:
     - `/personalized-risk` endpoint (POST with user_profile + context)
     - `/compare-places` endpoint (POST with multiple places)
     - `/safe-times` endpoint (GET with location + user_profile)
     - Add confidence scores to all responses

7. **geo_utils.py** (191 lines) - PRODUCTION READY ✅
   - DONE:
     - CITY_NAME_MAPPINGS for NYC, London, Chicago (extensible)
     - detect_city_from_address() - Extract city from geocoded address
     - detect_city_from_coordinates() - Reverse geocoding
     - get_coordinates_with_city() - Geocode + city detection
     - get_coordinates() - Backward-compatible geocoding
     - haversine() - Distance calculation
     - find_nearby_points() - Bounding box + precise distance filtering
   - TODO:
     - None (module is complete for current needs)

8. **utils.py** (249 lines) - PRODUCTION READY ✅
   - DONE:
     - create_safety_map() - Folium map with markers, clusters, heatmap
     - get_demographic_context() - Amenity-type-specific context
     - generate_safety_report() - Detailed text report with demographic analysis
     - Demographic-specific recommendations (children, elderly, nightlife)
   - TODO:
     - None (module is complete for current needs)

9. **model_evaluation.py** (291 lines) - PRODUCTION READY ✅
   - DONE:
     - evaluate_clustering_quality() - Silhouette, Davies-Bouldin, Calinski-Harabasz
     - evaluate_prediction_accuracy() - Zone prediction and safety score consistency
     - generate_evaluation_report() - Comprehensive model assessment
   - TODO:
     - None (module is complete for current needs)

10. **(future) llm_service.py** - NOT STARTED ❌
    - TODO:
      - OpenAI/Claude API integration
      - Prompt templates for safety explanations
      - Explanation caching (Redis or in-memory)
      - Context-aware recommendation generation

11. **(future) db_config.py** - NOT STARTED ❌
    - TODO:
      - PostgreSQL connection pooling
      - get_nearby_crimes() - Spatial query with ST_DWithin
      - calculate_risk_score() - Database-backed risk calculation
      - get_demographic_weights() - Load from demographic_impact_weights table
      - insert_crime_incident() - Batch insertion helper

FRONTEND FILES TO MODIFY:

1. Map component
   - ADD: Crime-type layer toggles
   - ADD: Confidence visualization
   - ADD: Time slider

2. NEW: PersonalizedRiskForm component
   - ADD: User profile input
   - ADD: Context selection

3. NEW: PlaceComparison component
   - ADD: Side-by-side comparison UI

================================================================================
PART 8: QUICK WINS (Implement This Week)
================================================================================

1. REPLACE FAKE AMENITY DATA (1 day)
   - Use Overpass API to get real bars, parks, transit stops
   - Immediate improvement to data quality

2. ADD CRIME CATEGORY SEPARATION (1 day)
   - Split analysis into: sexual, harassment, assault, theft, property
   - Foundation for demographic impact matrix

3. ADD BASIC CONFIDENCE SCORE (1 day)
   - confidence = f(sample_size, recency)
   - Show users how reliable scores are

4. CREATE DEMOGRAPHIC IMPACT MATRIX CONFIG (1 day)
   - JSON config file with weights
   - Easy to tune without code changes

5. ADD TIME-DECAY TO EXISTING MODELS (1 day)
   - Recent crimes weighted higher
   - More relevant predictions

================================================================================
PART 9: RESEARCH CITATIONS & JUSTIFICATION
================================================================================

Architecture justified by:
- ST-GCN: "Spatio-Temporal Graph Convolutional Networks" (AAAI 2018)
- Ada-GCNLSTM: Adaptive cross-city crime prediction (2024)
- Demographic impact: FBI Victimization Statistics methodology
- Uncertainty: Bayesian Deep Learning for crime prediction (2023)

Key papers to cite:
1. "Crime Prediction Using Machine Learning" - IEEE Smart Cities 2024
2. "Spatiotemporal Crime Hotspot Detection" - ACM SIGSPATIAL 2023
3. "Ethical Considerations in Predictive Policing" - Nature 2022

================================================================================
END OF RESEARCH 3
================================================================================


